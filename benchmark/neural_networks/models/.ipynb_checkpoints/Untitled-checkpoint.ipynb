{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0912dab-530e-4d90-a7db-2427267fe738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@128, 1s, eegnet: torch.Size([16, 4])\n",
      "eegconformer: torch.Size([16, 4])\n",
      "deepconvnet: torch.Size([16, 4])\n",
      "ctnnet: torch.Size([16, 4])\n",
      "@128, 3s, eegnet: torch.Size([16, 4])\n",
      "eegconformer: torch.Size([16, 4])\n",
      "deepconvnet: torch.Size([16, 4])\n",
      "ctnnet: torch.Size([16, 4])\n",
      "@256, 1s, eegnet: torch.Size([16, 4])\n",
      "eegconformer: torch.Size([16, 4])\n",
      "deepconvnet: torch.Size([16, 4])\n",
      "ctnnet: torch.Size([16, 4])\n",
      "@256, 3s, eegnet: torch.Size([16, 4])\n",
      "eegconformer: torch.Size([16, 4])\n",
      "deepconvnet: torch.Size([16, 4])\n",
      "ctnnet: torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "#### Sanity check on all models\n",
    "from eegnet import EEGNet\n",
    "from conformer import Conformer\n",
    "from deepconvnet import DeepConvNet\n",
    "from ctnnet import EEGTransformer as CTNNet\n",
    "from EEGPT import LitEEGPTModel\n",
    "from biot import BIOTClassifier\n",
    "from bendr import BendrClassifier\n",
    "from cbramod import CBraModClassifier\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "fs=[128, 256]\n",
    "data_length=[1,3]\n",
    "for sampling_rate in fs:\n",
    "    for data_l in data_length:  \n",
    "        window_length = int(data_l*sampling_rate)\n",
    "        eegnet = EEGNet(no_spatial_filters=4, no_channels=62, no_temporal_filters=8, temporal_length_1=int(sampling_rate/2), temporal_length_2=int(sampling_rate/128)*16, window_length=window_length, num_class=4, drop_out_ratio=0.50, pooling2=int(sampling_rate/32), pooling3=8)\n",
    "        out1 = eegnet(torch.randn(16,62,window_length))\n",
    "        print(f\"@{sampling_rate}, {data_l}s, eegnet:\", out1.shape)\n",
    "        \n",
    "        eegconformer = Conformer(num_channel=62, data_length=window_length, emb_size=40, depth=6, n_classes=4)\n",
    "        out2 = eegconformer(torch.randn(16,62,window_length))\n",
    "        print(\"eegconformer:\", out2.shape)\n",
    "        \n",
    "        deepconvnet = DeepConvNet(number_channel=62, nb_classes=4, dropout_rate=0.5, sampling_rate=sampling_rate, data_length=window_length)\n",
    "        out3 = deepconvnet(torch.randn(16,62,window_length))\n",
    "        print(\"deepconvnet:\",out3.shape)\n",
    "        \n",
    "        ctnnet = CTNNet(heads=4, emb_size=40, depth=6, number_class=4, number_channel=62, data_length=window_length, sampling_rate=sampling_rate)\n",
    "        out4 = ctnnet(torch.randn(16,62,window_length))\n",
    "        print(\"ctnnet:\",out4.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c947a-b324-432f-b4b5-ab59f2c787e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eegpt = LitEEGPTModel(load_path=\"/lustre1/project/stg_00160/eegpt/EEGPT/checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\",chans_num=19, num_class=7, data_length=int(256*3))\n",
    "\n",
    "biot = BIOTClassifier(\n",
    "    input_eeg_channel=62,\n",
    "    emb_size=256,\n",
    "    heads=8,\n",
    "    depth=4,\n",
    "    n_classes=4,\n",
    "    n_fft=200,\n",
    "    hop_length=100,\n",
    "    n_channels=18\n",
    ")\n",
    "biot.biot.load_state_dict(torch.load(\"/vsc-hard-mounts/leuven-data/343/vsc34340/BIOT-main/pretrained-models/EEG-six-datasets-18-channels.ckpt\"))\n",
    "\n",
    "bendr = BendrClassifier(num_class=4, num_channels=61, data_length=int(256*3), pre_trained_model_path=\"/lustre1/project/stg_00160/bendr/encoder.pt\")\n",
    "\n",
    "cbramod = CBraModClassifier(num_class=4, num_channel=61, data_length=int(200*3), pretrained_dir=\"/lustre1/project/stg_00160/cbramod/pretrained_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20840295-3804-4852-86d7-d54050ac6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = eegnet(torch.randn(16,62,384))\n",
    "out2 = eegconformer(torch.randn(16,62,384))\n",
    "out3 = deepconvnet(torch.randn(16,62,256))\n",
    "out4 = ctnnet(torch.randn(16,62,384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ce5bec-415b-41e0-9cec-3c3723226973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "out = cbramod(torch.randn(16,61,3,200))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a6b4ac-9385-400d-8366-a6ac778d052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vsc-hard-mounts/leuven-data/343/vsc34340/miniconda3/envs/torch_env/lib/python3.8/site-packages/torch/functional.py:665: UserWarning: A window was not provided. A rectangular window will be applied,which is known to cause spectral leakage. Other windows such as torch.hann_window or torch.hamming_window can are recommended to reduce spectral leakage.To suppress this warning and use a rectangular window, explicitly set `window=torch.ones(n_fft, device=<device>)`. (Triggered internally at ../aten/src/ATen/native/SpectralOps.cpp:836.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "out5 = biot(torch.randn(16,62,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170f4eaa-55a1-4feb-835e-87ca5809d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "out1 = eegnet(torch.randn(16,62,384))\n",
    "out2 = eegconformer(torch.randn(16,62,384))\n",
    "out3 = deepconvnet(torch.randn(16,62,384))\n",
    "out4 = ctnnet(torch.randn(16,62,384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f0b8dd-367d-437f-b3dc-3e232b3d1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4]) torch.Size([16, 4]) torch.Size([16, 4]) torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "print(out1.shape, out2.shape, out3.shape, out4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f84f28-b0a5-4006-97a5-ff0332e15dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chan_idx = np.array(range(19))\n",
    "h = eegpt(torch.randn(16,19,int(256*3)),torch.from_numpy(chan_idx).type(torch.IntTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212662ec-89c5-4799-8c22-fc142d9ba017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep  31 channels for EEGPT\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(\"/vsc-hard-mounts/leuven-data/343/vsc34340/new_eeg_mae/util/dataset_specs.yaml\", 'r') as f:\n",
    "    dataset_yaml = yaml.safe_load(f)\n",
    "downstream_channels = dataset_yaml['upper_limb_motorexecution']['chan_names']\n",
    "eegpt_channels = [      'FP1', 'FPZ', 'FP2', \n",
    "                        \"AF7\", 'AF3', 'AF4', \"AF8\", \n",
    "            'F7', 'F5', 'F3', 'F1', 'FZ', 'F2', 'F4', 'F6', 'F8', \n",
    "        'FT7', 'FC5', 'FC3', 'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8', \n",
    "            'T7', 'C5', 'C3', 'C1', 'CZ', 'C2', 'C4', 'C6', 'T8', \n",
    "        'TP7', 'CP5', 'CP3', 'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8',\n",
    "             'P7', 'P5', 'P3', 'P1', 'PZ', 'P2', 'P4', 'P6', 'P8', \n",
    "                      'PO7', \"PO5\", 'PO3', 'POZ', 'PO4', \"PO6\", 'PO8', \n",
    "                               'O1', 'OZ', 'O2', ]\n",
    "\n",
    "eegpt_channels_lower = [ch.lower() for ch in eegpt_channels]\n",
    "\n",
    "channels_keep = []\n",
    "eegpt_channel_idx = []\n",
    "# Filter standard channels that exist in montage\n",
    "for ch_idx, ch in enumerate(downstream_channels):\n",
    "    ch_lower = ch.lower()\n",
    "    if ch_lower in eegpt_channels_lower:\n",
    "        # Exact match\n",
    "        eegpt_channel_idx.append( eegpt_channels_lower.index(ch_lower))\n",
    "        channels_keep.append(ch_idx)\n",
    "print(\"keep \", len(channels_keep), \"channels for EEGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff26510e-d1a5-4e65-8392-8b1440019386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_channels_from_available_channels(downstream_channels, available_channels):\n",
    "    available_channels_lower = [ch.lower() for ch in available_channels]\n",
    "    channels_keep = []\n",
    "    available_channel_idx = []\n",
    "    for ch_idx, ch in enumerate(downstream_channels):\n",
    "        ch_lower = ch.lower()\n",
    "        if ch_lower in available_channels_lower:\n",
    "            # Exact match\n",
    "            available_channel_idx.append( available_channels_lower.index(ch_lower))\n",
    "            channels_keep.append(ch_idx)\n",
    "    return "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_torch_env",
   "language": "python",
   "name": "new_torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
